{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0FQnJJC1S-oi",
   "metadata": {
    "id": "0FQnJJC1S-oi"
   },
   "source": [
    "# RAG pipeline with Chroma, Sentence-Transformers, and FLAN-T5 (Colab-ready)\n",
    "\n",
    "This Colab notebook implements:\n",
    "- OCR-aware PDF extraction (handles flattened PDFs) using `pdfplumber` + `pytesseract`.\n",
    "- Text extraction from DOCX and images.\n",
    "- Chunking and per-document JSON export.\n",
    "- Embeddings with `sentence-transformers` (recommended: `all-mpnet-base-v2`).\n",
    "- Vector storage in Chroma (local persistent folder).\n",
    "- Retrieval (query → embedding → top-K).\n",
    "- RAG answer synthesis using `google/flan-t5-base`.\n",
    "- Optional exact-span extraction using a QA model.\n",
    "\n",
    "**How to use**\n",
    "1. Open this notebook in Google Colab.\n",
    "2. (Optional) Change runtime to GPU for faster generation: Runtime → Change runtime type → GPU.\n",
    "3. Run cells sequentially. Upload files when prompted (or mount Google Drive and modify paths).\n",
    "4. Use the `rag_answer` function to ask questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-OiwxioCS-ok",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58219,
     "status": "ok",
     "timestamp": 1764728193450,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "-OiwxioCS-ok",
    "outputId": "3787e0a5-e7c9-4016-9c0a-8a3cc5a7ddbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Connection timed out while downloading.\n",
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-unpack-jgi0u2n_\\\\pillow-12.0.0-cp313-cp313-win_amd64.whl'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (Colab)\n",
    "!pip install -q sentence-transformers chromadb pdfplumber python-docx pytesseract pillow transformers accelerate sentencepiece\n",
    "# install tesseract binary (Colab / Debian) and poppler-utils for PDF rasterization\n",
    "!apt-get update -qq && apt-get install -y -qq tesseract-ocr libtesseract-dev poppler-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5rxZ48MS-ok",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41808,
     "status": "ok",
     "timestamp": 1764728269385,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "a5rxZ48MS-ok",
    "outputId": "0b23036d-2bc8-469f-e4ce-6c747531747f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: all-mpnet-base-v2\n",
      "Generation model: google/flan-t5-base\n",
      "DB_DIR: /content/chroma_db\n"
     ]
    }
   ],
   "source": [
    "# Imports, paths, and settings\n",
    "import os, json, uuid\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Text extraction libs\n",
    "import pdfplumber, docx\n",
    "from PIL import Image, ImageFilter, ImageOps\n",
    "import pytesseract\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Chroma\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Generation & QA\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Folders\n",
    "DB_DIR = \"/content/chroma_db\"\n",
    "JSON_OUTPUT_DIR = \"/content/doc_jsons\"\n",
    "os.makedirs(DB_DIR, exist_ok=True)\n",
    "os.makedirs(JSON_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Embedding model (choose accuracy vs speed)\n",
    "EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"   # recommended for accuracy\n",
    "# EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"  # alternative (faster)\n",
    "\n",
    "# Generation model (RAG fusion / answer synthesis)\n",
    "GEN_MODEL_NAME = \"google/flan-t5-base\"  # use GPU for larger models\n",
    "\n",
    "# QA span-extraction model (optional)\n",
    "QA_MODEL_NAME = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# Tesseract settings\n",
    "pytesseract.pytesseract.tesseract_cmd = \"/usr/bin/tesseract\"  # colab default\n",
    "OCR_LANG = \"eng\"\n",
    "TESSERACT_COMMON_CONFIG = f\"--oem 1 --psm 6 -l {OCR_LANG}\"\n",
    "\n",
    "print(\"Embedding model:\", EMBEDDING_MODEL_NAME)\n",
    "print(\"Generation model:\", GEN_MODEL_NAME)\n",
    "print(\"DB_DIR:\", DB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "OggH3Sb3S-ol",
   "metadata": {
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1764728274804,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "OggH3Sb3S-ol"
   },
   "outputs": [],
   "source": [
    "# OCR-aware PDF extractor and helpers\n",
    "\n",
    "def preprocess_pil_image_for_ocr(pil_img):\n",
    "    img = pil_img.convert(\"L\")\n",
    "    img = ImageOps.autocontrast(img, cutoff=1)\n",
    "    img = img.filter(ImageFilter.MedianFilter(size=3))\n",
    "    return img\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, ocr_if_needed=True, min_text_len_for_layer=50, dpi=300):\n",
    "    all_text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_index, page in enumerate(pdf.pages):\n",
    "            try:\n",
    "                text = page.extract_text()\n",
    "            except Exception as e:\n",
    "                print(f\"page.extract_text() error on page {page_index+1}: {e}\")\n",
    "                text = None\n",
    "\n",
    "            if (not text or len(text.strip()) < min_text_len_for_layer) and ocr_if_needed:\n",
    "                print(f\"Page {page_index+1}: Running OCR at {dpi} DPI...\")\n",
    "                try:\n",
    "                    page_image = page.to_image(resolution=dpi).original\n",
    "                except Exception as e:\n",
    "                    print(f\"page.to_image error fallback for page {page_index+1}: {e}\")\n",
    "                    page_image = page.to_image().original\n",
    "                page_image = preprocess_pil_image_for_ocr(page_image)\n",
    "                try:\n",
    "                    text = pytesseract.image_to_string(page_image, config=TESSERACT_COMMON_CONFIG)\n",
    "                except Exception as e:\n",
    "                    print(f\"Tesseract OCR failed on page {page_index+1}: {e}\")\n",
    "                    text = \"\"\n",
    "            if not text:\n",
    "                text = \"\"\n",
    "            all_text.append(f\"\\\\n--- PAGE {page_index+1} ---\\\\n{text}\")\n",
    "    return \"\\\\n\".join(all_text)\n",
    "\n",
    "def extract_text_from_docx(path):\n",
    "    try:\n",
    "        doc = docx.Document(path)\n",
    "        paragraphs = [p.text for p in doc.paragraphs if p.text and p.text.strip()]\n",
    "        return \"\\\\n\".join(paragraphs)\n",
    "    except Exception as e:\n",
    "        print(\"DOCX read error:\", e)\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_image(path):\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        img = preprocess_pil_image_for_ocr(img)\n",
    "        return pytesseract.image_to_string(img, config=TESSERACT_COMMON_CONFIG)\n",
    "    except Exception as e:\n",
    "        print(\"Image OCR error:\", e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jSLKi2K6S-ol",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "executionInfo": {
     "elapsed": 14133,
     "status": "ok",
     "timestamp": 1764728931694,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "jSLKi2K6S-ol",
    "outputId": "4e158aa7-f2b6-40a7-d015-4bdf45e77754"
   },
   "outputs": [],
   "source": [
    "# Upload files (Colab interactive)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "uploaded_filenames = list(uploaded.keys())\n",
    "print(\"Uploaded:\", uploaded_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Mkx6T-8MS-ol",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1334,
     "status": "ok",
     "timestamp": 1764728937018,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "Mkx6T-8MS-ol",
    "outputId": "2a0405ba-be6d-4ac0-f761-900f5515bffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote JSON: /content/doc_jsons/Uniform Residential Loan Application details.json\n",
      "Total chunks prepared: 2\n"
     ]
    }
   ],
   "source": [
    "# Chunking, process files to JSON and prepare records\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk_tokens = tokens[i:i+chunk_size]\n",
    "        chunks.append(\" \".join(chunk_tokens))\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def process_and_export(files_list):\n",
    "    records = []\n",
    "    for fname in files_list:\n",
    "        path = Path(fname)\n",
    "        ext = path.suffix.lower()\n",
    "        if ext == \".pdf\":\n",
    "            raw = extract_text_from_pdf(str(path))\n",
    "        elif ext in [\".docx\", \".doc\"]:\n",
    "            raw = extract_text_from_docx(str(path))\n",
    "        elif ext in [\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\"]:\n",
    "            raw = extract_text_from_image(str(path))\n",
    "        else:\n",
    "            print(\"Skipping unsupported:\", fname)\n",
    "            continue\n",
    "\n",
    "        if not raw or len(raw.strip()) == 0:\n",
    "            print(\"No text extracted for:\", fname)\n",
    "            continue\n",
    "\n",
    "        metadata = {\"source_filename\": fname, \"id\": str(uuid.uuid4()), \"n_chars\": len(raw)}\n",
    "        chunks = chunk_text(raw, chunk_size=300, overlap=50)\n",
    "        doc_json = {\"metadata\": metadata, \"full_text\": raw, \"chunks\": []}\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"{metadata['id']}_chunk_{idx}\"\n",
    "            doc_json[\"chunks\"].append({\"chunk_id\": chunk_id, \"text\": chunk, \"chunk_index\": idx})\n",
    "            records.append({\"id\": chunk_id, \"text\": chunk, \"metadata\": {**metadata, \"chunk_index\": idx}})\n",
    "        outpath = Path(JSON_OUTPUT_DIR) / (path.stem + \".json\")\n",
    "        with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc_json, f, ensure_ascii=False, indent=2)\n",
    "        print(\"Wrote JSON:\", outpath)\n",
    "    return records\n",
    "\n",
    "# Run processing on uploaded files\n",
    "records = process_and_export(uploaded_filenames)\n",
    "print(\"Total chunks prepared:\", len(records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "A2M2rmoKS-ol",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5774,
     "status": "ok",
     "timestamp": 1764728946748,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "A2M2rmoKS-ol",
    "outputId": "20682500-bf85-48a1-a930-15a02bb79a08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 1/1 [00:03<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings vectors: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model and create embeddings\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "texts = [r[\"text\"] for r in records]\n",
    "ids = [r[\"id\"] for r in records]\n",
    "metadatas = [r[\"metadata\"] for r in records]\n",
    "\n",
    "BATCH = 64\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(texts), BATCH), desc=\"Embedding\"):\n",
    "    batch_texts = texts[i:i+BATCH]\n",
    "    embs = embed_model.encode(batch_texts, show_progress_bar=False, convert_to_numpy=True)\n",
    "    embeddings.extend(embs)\n",
    "print(\"Embeddings vectors:\", len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2HurUfuLS-ol",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1764728951559,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "2HurUfuLS-ol",
    "outputId": "f7c631ff-7082-4ff8-be79-706c38940ad8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma persisted at /content/chroma_db\n"
     ]
    }
   ],
   "source": [
    "# Create Chroma DB and insert vectors\n",
    "client = chromadb.PersistentClient(path=DB_DIR, settings=Settings())\n",
    "collection_name = \"customer_docs\"\n",
    "try:\n",
    "    collection = client.get_or_create_collection(collection_name)\n",
    "except Exception:\n",
    "    collection = client.create_collection(name=collection_name)\n",
    "\n",
    "# prepare embeddings to lists of floats (Chroma expects python floats)\n",
    "vecs = [e.tolist() if hasattr(e, \"tolist\") else list(map(float, e)) for e in embeddings]\n",
    "\n",
    "collection.add(ids=ids, metadatas=metadatas, documents=texts, embeddings=vecs)\n",
    "print(\"Chroma persisted at\", DB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "gjmTROOxS-ol",
   "metadata": {
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1764728955753,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "gjmTROOxS-ol"
   },
   "outputs": [],
   "source": [
    "# Retrieval helper\n",
    "def search_chroma(query_text, top_k=3):\n",
    "    q_emb = embed_model.encode([query_text], convert_to_numpy=True)[0].astype(float).tolist()\n",
    "    results = collection.query(query_embeddings=[q_emb], n_results=top_k, include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    hits = []\n",
    "    for i in range(len(results[\"ids\"][0])):\n",
    "        hits.append({\n",
    "            \"id\": results[\"ids\"][0][i],\n",
    "            \"document\": results[\"documents\"][0][i],\n",
    "            \"metadata\": results[\"metadatas\"][0][i],\n",
    "            \"distance\": results[\"distances\"][0][i]\n",
    "        })\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "MN2IRVRIS-om",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1761,
     "status": "ok",
     "timestamp": 1764728960879,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "MN2IRVRIS-om",
    "outputId": "5d4ed99a-3aa6-432a-ad44-c9e8df572409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load generation model (FLAN-T5) for RAG fusion\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Generation device:\", device)\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL_NAME).to(device)\n",
    "\n",
    "def tokenize_len(text):\n",
    "    return len(gen_tokenizer.encode(text, truncation=False))\n",
    "def truncate_context_by_tokens(chunks_texts, max_input_tokens, sep=\"\\n\\n---\\n\\n\"):\n",
    "    out = []\n",
    "    tokens = 0\n",
    "    for t in chunks_texts:\n",
    "        t_tokens = len(gen_tokenizer.encode(t + sep, truncation=False))\n",
    "        if tokens + t_tokens > max_input_tokens:\n",
    "            break\n",
    "        out.append(t)\n",
    "        tokens += t_tokens\n",
    "    return sep.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Z2ghJkt2S-om",
   "metadata": {
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1764728965586,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "Z2ghJkt2S-om"
   },
   "outputs": [],
   "source": [
    "# Prompt template and RAG answer function\n",
    "PROMPT_SYSTEM = \"\"\"You are a helpful assistant. Answer the user question using ONLY the CONTEXT provided.\n",
    "If the context does not contain enough information to answer, respond: INSUFFICIENT_CONTEXT.\n",
    "Be concise (max ~120 words). Provide the answer and then list the sources (filename and chunk index).\"\"\"\n",
    "\n",
    "PROMPT_USER_TEMPLATE = \"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1) Give a short direct answer (<120 words).\n",
    "2) After the answer print a \"SOURCES:\" section listing each source as - filename (chunk_index).\n",
    "3) If you can't answer from the context, respond exactly: INSUFFICIENT_CONTEXT\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(context, question):\n",
    "    return PROMPT_SYSTEM + \"\\n\\n\" + PROMPT_USER_TEMPLATE.format(context=context, question=question)\n",
    "\n",
    "def rag_answer(question, top_k=3, max_context_tokens=1500, max_answer_tokens=180):\n",
    "    # 1. Retrieve\n",
    "    hits = search_chroma(question, top_k=top_k)\n",
    "    if not hits:\n",
    "        return {\"question\": question, \"answer\": \"INSUFFICIENT_CONTEXT\", \"provenance\": [], \"used_context\": \"\"}\n",
    "\n",
    "    # 2. Build entries with provenance\n",
    "    entries = []\n",
    "    provenance = []\n",
    "    for h in hits:\n",
    "        src = h['metadata'].get('source_filename', 'unknown')\n",
    "        idx = h['metadata'].get('chunk_index', -1)\n",
    "        entry_text = f\"[{src} | chunk {idx}]\\n{h['document']}\"\n",
    "        entries.append(entry_text)\n",
    "        provenance.append({\"source\": src, \"chunk_index\": idx, \"distance\": h['distance'], \"id\": h['id']})\n",
    "\n",
    "    # 3. Truncate context to token budget\n",
    "    context = truncate_context_by_tokens(entries, max_context_tokens, sep=\"\\n\\n---\\n\\n\")\n",
    "    prompt = build_prompt(context, question)\n",
    "\n",
    "    # 4. Tokenize & generate\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
    "    outputs = gen_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_answer_tokens,\n",
    "        num_beams=4,\n",
    "        do_sample=False,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    answer = gen_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    if not answer:\n",
    "        answer = \"INSUFFICIENT_CONTEXT\"\n",
    "\n",
    "    return {\"question\": question, \"answer\": answer, \"provenance\": provenance, \"used_context\": context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3fuIWaeS-om",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1623,
     "status": "ok",
     "timestamp": 1764728970514,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "d3fuIWaeS-om",
    "outputId": "bffba6b5-234e-4d98-c78d-cb92da4172b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Optional: QA span extraction for exact fields\n",
    "qa_pipeline = pipeline(\"question-answering\", model=QA_MODEL_NAME, tokenizer=QA_MODEL_NAME, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def extract_exact_span(question, top_hit):\n",
    "    context = top_hit['document']\n",
    "    res = qa_pipeline(question=question, context=context, top_k=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "E706mvpYS-om",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12288,
     "status": "ok",
     "timestamp": 1764729254561,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "E706mvpYS-om",
    "outputId": "d4e68f8a-e87d-41e1-8a6d-b6e18f48e6ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIT 1: Customer information.docx chunk 0\n",
      "Page 1: Borrower Information\\nSection 1a. Personal Information \\nName: James A. Smith \\nSocial Security Number: 000-XX-1234 \\nDate of Birth: 06/15/1985 \\nCitizenship: Check Box: U.S. Citizen \\nMarital Status: Check Box: Married \\nDependents: 1 | Ages: 5 \\nCurrent Address: 4509 Maple Drive \\nCity/State/Zip: Austin, TX 78701 \\nHousing: Check Rent ($1,800/month) \\nSection 1b. Employment/Income \\nEmployer Name: Tech Solutions Inc. \\nGross Monthly Income (Base): $8,500 \\nBonus: $500 \\nTotal Income: $\n",
      "distance: 1.2032997608184814\n",
      "-----\n",
      "\n",
      "HIT 2: Uniform Residential Loan Application details.docx chunk 0\n",
      "Uniform Residential Loan Application Summary 3\\nBorrower Name: JACK MERIDITH SPECTOR 4444Lender Loan No/Universal Loan Identifier: US BANK 5Agency Case No.: US-100 6Date of Application Signature: 11/10/2025 7\\nSection 1: Borrower Information 8\\n1a. Personal Information\\nSocial Security Number: 15 52 556 (or Individual Taxpayer Identification Number) 9999\\nDate of Birth (mm/dd/yyyy): 02/02/2000 10\\nCitizenship: U.S. Citizen 11\\nType of Credit: Individual credit 12\\nOther Borrowers: NA 13\\nMarital\n",
      "distance: 1.372542381286621\n",
      "-----\n",
      "\n",
      "HIT 3: Uniform Residential Loan Application details.docx chunk 1\n",
      "real estate42.\\nSections 3a, 3b, and 3c do not apply434343434343434343.\\nSection 4: Loan and Property Information 44\\n4a. Loan and Property Information\\nLoan Amount: $ 200000.00 45\\nLoan Purpose: Purchase 46\\nProperty Value: $ 500000.00 47\\nProperty Address: BAKERS STREET Unit # 5, NEWYORK, NC, ZIP 12563, USA 48484848\\nNumber of Units: 55 49\\nOccupancy: Investment Property 50\\nMixed-Use Property (Business Space): NO 51\\nManufactured Home: NO 52\\n4b. Other New Mortgage Loans\\nDoes not apply53.\\n4\n",
      "distance: 1.4705379009246826\n",
      "-----\n",
      "\n",
      "ANSWER:\n",
      " [Customer information.docx | chunk 0]\n",
      "\n",
      "PROVENANCE:\n",
      " [{'source': 'Customer information.docx', 'chunk_index': 0, 'distance': 1.2032997608184814, 'id': '90b48738-aca5-42be-997f-c89476ea3bd4_chunk_0'}, {'source': 'Uniform Residential Loan Application details.docx', 'chunk_index': 0, 'distance': 1.372542381286621, 'id': '62852365-0f32-4766-abdb-d561360b3e13_chunk_0'}, {'source': 'Uniform Residential Loan Application details.docx', 'chunk_index': 1, 'distance': 1.4705379009246826, 'id': '62852365-0f32-4766-abdb-d561360b3e13_chunk_1'}]\n",
      "\n",
      "Exact span extracted by QA model: {'score': 3.269751914558583e-06, 'start': 462, 'end': 468, 'answer': '$8,500'}\n"
     ]
    }
   ],
   "source": [
    "# Examples / Usage\n",
    "\n",
    "# Retrieval-only example:\n",
    "q = \"list of all the customers with monthly income greater than 5000 dollars?\"\n",
    "hits = search_chroma(q, top_k=3)\n",
    "for i,h in enumerate(hits,1):\n",
    "    print(f\"HIT {i}: {h['metadata']['source_filename']} chunk {h['metadata']['chunk_index']}\")\n",
    "    print(h['document'][:500].replace(\"\\n\",\" \"))\n",
    "    print(\"distance:\", h['distance'])\n",
    "    print(\"-----\\n\")\n",
    "\n",
    "# RAG: generate concise, grounded answer\n",
    "out = rag_answer(q, top_k=3)\n",
    "print(\"ANSWER:\\n\", out[\"answer\"])\n",
    "print(\"\\nPROVENANCE:\\n\", out[\"provenance\"])\n",
    "\n",
    "# Optional: exact span from top hit\n",
    "if hits:\n",
    "    span = extract_exact_span(q, hits[0])\n",
    "    print(\"\\nExact span extracted by QA model:\", span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FwiwJIc_S-om",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1764700402264,
     "user": {
      "displayName": "Hariom Ramkrishna",
      "userId": "08131443419838177211"
     },
     "user_tz": -330
    },
    "id": "FwiwJIc_S-om",
    "outputId": "ef064e64-d328-462c-8adb-72590fd38cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped at /content/chroma_db.zip and /content/doc_jsons.zip\n"
     ]
    }
   ],
   "source": [
    "# Save DB and JSONs for download (optional)\n",
    "!zip -r -q /content/chroma_db.zip /content/chroma_db\n",
    "!zip -r -q /content/doc_jsons.zip /content/doc_jsons\n",
    "print(\"Zipped at /content/chroma_db.zip and /content/doc_jsons.zip\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
